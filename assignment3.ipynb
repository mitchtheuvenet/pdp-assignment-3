{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM97Xlib8dGky7eWsXuQDf2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitchtheuvenet/pdp-assignment-3/blob/master/assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWkPnB0D6AKr",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwl2YOZIn0TV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop3.2.tgz\n",
        "!tar -xvf spark-3.0.0-preview2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUB4QOlWtDXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview2-bin-hadoop3.2\""
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkkT1Rr5oszn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0g1FGxJo0FH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "  .master('local') \\\n",
        "  .appName('Assignment 3 Titanic') \\\n",
        "  .config('spark.executor.memory', '1gb') \\\n",
        "  .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "884DI7SlpaG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd = sc.textFile('titanic.csv')\n",
        "rdd = rdd.map(lambda line: line.split(','))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEtCAQn_py_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "df = rdd.map(lambda line: Row(survived=line[0],\n",
        "                              p_class=line[1],\n",
        "                              name=line[2],\n",
        "                              sex=line[3],\n",
        "                              age=line[4],\n",
        "                              siblings_spouses_aboard=line[5],\n",
        "                              parents_children_aboard=line[6],\n",
        "                              fare=line[7])).toDF()\n",
        "\n",
        "df = df.withColumn('age', df['age'].cast(IntegerType()))\n",
        "df = df.withColumn('fare', df['fare'].cast(FloatType()))\n",
        "df = df.withColumn('survived', df['survived'].cast(BooleanType()))\n",
        "\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe2iIKJdxpzB",
        "colab_type": "text"
      },
      "source": [
        "## 3a. Calculate the conditional probability that a person survives, given their sex and passenger class:\n",
        "\n",
        "*P(S = true | G = female, C = 1)*\n",
        "\n",
        "*P(S = true | G = female, C = 2)*\n",
        "\n",
        "*P(S = true | G = female, C = 3)*\n",
        "\n",
        "*P(S = true | G = male, C = 1)*\n",
        "\n",
        "*P(S = true | G = male, C = 2)*\n",
        "\n",
        "*P(S = true | G = male, C = 3)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQeQvv7HyRB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calcSurvivalP(df, sex, p_class):\n",
        "  passengers = df.rdd.filter(lambda line: line['sex'] == sex and line['p_class'] == p_class)\n",
        "  passenger_count = (spark.createDataFrame(passengers)).count()\n",
        "\n",
        "  survivors = passengers.filter(lambda line: line['survived'] == True)\n",
        "  survivor_count = (spark.createDataFrame(survivors)).count()\n",
        "\n",
        "  return str(survivor_count) + '/' + str(passenger_count)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY9V6PHQ1XM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('P(S = true | G = female, C = 1) = ' + calcSurvivalP(df, 'female', '1'))\n",
        "print('P(S = true | G = female, C = 2) = ' + calcSurvivalP(df, 'female', '2'))\n",
        "print('P(S = true | G = female, C = 3) = ' + calcSurvivalP(df, 'female', '3'))\n",
        "print('P(S = true | G = male, C = 1) = ' + calcSurvivalP(df, 'male', '1'))\n",
        "print('P(S = true | G = male, C = 2) = ' + calcSurvivalP(df, 'male', '2'))\n",
        "print('P(S = true | G = male, C = 3) = ' + calcSurvivalP(df, 'male', '3'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjHhOGNF45xJ",
        "colab_type": "text"
      },
      "source": [
        "## 3b. What is the probability that a child who is in third class and is 10 years old or younger survives? Since the number of data points that satisfy the condition is small, use the \"bayesian\" approach and represent your probability as a beta distribution. Calculate a belief distribution for:\n",
        "\n",
        "*S = true | A <= 10, C = 3*\n",
        "\n",
        "## You can express your answer as a parameterized distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PiNCyGBSt2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calcSurvivalPChild(df):\n",
        "  passengers = df.rdd.filter(lambda line: line['age'] <= 10 and line['p_class'] == '3')\n",
        "  passenger_count = (spark.createDataFrame(passengers)).count()\n",
        "\n",
        "  survivors = passengers.filter(lambda line: line['survived'] == True)\n",
        "  survivor_count = (spark.createDataFrame(survivors)).count()\n",
        "\n",
        "  deceased_count = passenger_count - survivor_count\n",
        "\n",
        "  return 'Beta(⍺ = ' + str(survivor_count) + ', β = ' + str(deceased_count) + ')'"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EJGC1l-Yf7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(calcSurvivalPChild(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_TzJ640FMbu",
        "colab_type": "text"
      },
      "source": [
        "## 3c. How much did people pay to be on the ship? Calculate the expectation of fare conditioned on class:\n",
        "\n",
        "*E[X | C = 1]*\n",
        "\n",
        "*E[X | C = 2]*\n",
        "\n",
        "*E[X | C = 3]*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaKD5lH1ILTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calcFarePerClassE(df, p_class):\n",
        "  df_filter = df.rdd.filter(lambda line: line['p_class'] == p_class)\n",
        "\n",
        "  avg_fare = (spark.createDataFrame(df_filter)).groupBy('p_class').avg().collect()\n",
        "\n",
        "  return str(avg_fare[0][2])"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaisqpzNLTga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('E[X | C = 1] = ' + calcFarePerClassE(df, '1'))\n",
        "print('E[X | C = 2] = ' + calcFarePerClassE(df, '2'))\n",
        "print('E[X | C = 3] = ' + calcFarePerClassE(df, '3'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMZbGoJWSOnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": 56,
      "outputs": []
    }
  ]
}